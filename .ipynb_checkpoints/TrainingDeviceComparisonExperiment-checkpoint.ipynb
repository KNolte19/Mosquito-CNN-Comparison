{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5588,
     "status": "ok",
     "timestamp": 1713272045476,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -120
    },
    "id": "1WNnQSTrXrf1",
    "outputId": "b950718a-5f3d-4094-b606-ce86817c0233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import keras_cv\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "from google.colab.patches import cv2_imshow\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "species_list = ['AE', 'AL', 'JA', 'KO']\n",
    "\n",
    "# on colab\n",
    "DIR_PATH = \"/content/drive/MyDrive/PhD/WingDatabase/DeviceDataset/{}\".format(DEPICTION)\n",
    "SAVE_DIR = \"/content/drive/MyDrive/PhD/WingComparisonStudy/models_device\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odEFldBWcSvW"
   },
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1713272049400,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -120
    },
    "id": "G8z0FVKRKTav"
   },
   "outputs": [],
   "source": [
    "# 3 Rings for the Elven-kings under the sky,\n",
    "# 7 for the Dwarf-lords in their halls of stone,\n",
    "# 9 for Mortal Men, doomed to die,\n",
    "# 1 for the Dark Lord on his dark throne\n",
    "SEED = 3\n",
    "DEPICTION = \"body\"\n",
    "\n",
    "IMG_SIZE = 300\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE_TRANSFER = 5e-4\n",
    "MAX_EPOCHS_TRANSFER = 24\n",
    "EARLY_STOP_PATIENCE = 12\n",
    "STEPS_PER_EPOCH = 81\n",
    "DROPOUT = 0.5\n",
    "\n",
    "AUG_LAYER = 3\n",
    "AUG_MAGNITUDE = .05\n",
    "AUG_ROTATTION = .05\n",
    "AUG_TRANSLATION = .05\n",
    "AUG_SHARPNESS = .25\n",
    "AUG_COLORDEGEN = .25\n",
    "AUG_NOISE = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XG5UQ2yzcQy8"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2201,
     "status": "ok",
     "timestamp": 1713272051596,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -120
    },
    "id": "0h79J_yKZ9FY"
   },
   "outputs": [],
   "source": [
    "def create_dict(data_split):\n",
    "    #get a list of image paths defined by the target list\n",
    "    image_path_dict = {}\n",
    "\n",
    "    for species in species_list:\n",
    "        path = DIR_PATH + \"/{}/{}/*.png\".format(data_split, species)\n",
    "        image_path_dict[species] = tf.data.Dataset.list_files(path,  shuffle=True, seed=SEED)\n",
    "\n",
    "    return image_path_dict\n",
    "\n",
    "def combine_dict(dataset_dict):\n",
    "  # combine all target_dicts together\n",
    "  for i,key in enumerate(dataset_dict.keys()):\n",
    "    if i == 0:\n",
    "        image_path_ds = dataset_dict[key]\n",
    "    else:\n",
    "        image_path_ds = image_path_ds.concatenate(dataset_dict[key])\n",
    "\n",
    "  return image_path_ds.shuffle(buffer_size=image_path_ds.cardinality())\n",
    "\n",
    "#create dicts with species as key and tf.ShuffleDataset containing paths as values for each device\n",
    "if DEPICTION == \"body\":\n",
    "  train_phone_path_dict = create_dict(\"train/phone\")\n",
    "train_macro_path_dict = create_dict(\"train/macro\")\n",
    "train_micro_path_dict = create_dict(\"train/micro\")\n",
    "\n",
    "val_path_dict = create_dict(\"val\")\n",
    "test_path_dict = create_dict(\"test\")\n",
    "\n",
    "#combine dicts to Tensorflow dataset\n",
    "if DEPICTION == \"body\":\n",
    "  trainDS_phone_filelist = combine_dict(train_phone_path_dict)\n",
    "trainDS_macro_filelist = combine_dict(train_macro_path_dict)\n",
    "trainDS_micro_filelist = combine_dict(train_micro_path_dict)\n",
    "\n",
    "valDS_filelist = combine_dict(val_path_dict)\n",
    "testDS_filelist = combine_dict(test_path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1713272051828,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -120
    },
    "id": "0b9mkQkcbnqS"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DEPICTION' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m     img \u001b[38;5;241m=\u001b[39m getImage(file_path)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img, species_label\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mDEPICTION\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     25\u001b[0m   trainDSphone_raw \u001b[38;5;241m=\u001b[39m trainDS_phone_filelist\u001b[38;5;241m.\u001b[39mmap(process_path, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[1;32m     26\u001b[0m trainDSmacro_raw \u001b[38;5;241m=\u001b[39m trainDS_macro_filelist\u001b[38;5;241m.\u001b[39mmap(process_path, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DEPICTION' is not defined"
     ]
    }
   ],
   "source": [
    "def getLabel_onehot(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    speciesName = tf.strings.split(file_path, os.path.sep)[-2]\n",
    "    # get one_hot vector boolean\n",
    "    species_one_hot = speciesName == species_list\n",
    "    # cast vector type to integer\n",
    "    species_one_hot = tf.cast(species_one_hot, dtype=tf.int8, name=None)\n",
    "    return species_one_hot\n",
    "\n",
    "def getImage(file_path):\n",
    "    # Load the .png image and cast it to float32\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    img = tf.cast(img, dtype=tf.float32)\n",
    "    return img\n",
    "\n",
    "def process_path(file_path):\n",
    "    # wrapper function to get image and label\n",
    "    species_label = getLabel_onehot(file_path)\n",
    "    img = getImage(file_path)\n",
    "\n",
    "    return img, species_label\n",
    "\n",
    "if DEPICTION == \"body\":\n",
    "  trainDSphone_raw = trainDS_phone_filelist.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "trainDSmacro_raw = trainDS_macro_filelist.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "trainDSmicro_raw = trainDS_micro_filelist.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "valDS_raw = valDS_filelist.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "testDS_raw = testDS_filelist.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-osu8ax2cXNU"
   },
   "source": [
    "## Initialise Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 7824,
     "status": "ok",
     "timestamp": 1713272059649,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -120
    },
    "id": "Z39pmGyD3JoG"
   },
   "outputs": [],
   "source": [
    "randAugment = keras_cv.layers.RandAugment(value_range=(0, 255),\n",
    "                                           augmentations_per_image=AUG_LAYER,\n",
    "                                           magnitude=AUG_MAGNITUDE,\n",
    "                                           geometric=False,\n",
    "                                           rate=1,\n",
    "                                           seed=SEED)\n",
    "\n",
    "# geometric augmentations\n",
    "randRot = tf.keras.layers.RandomRotation(AUG_ROTATTION, seed=SEED)\n",
    "randFlip = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=SEED)\n",
    "randTrans = tf.keras.layers.RandomTranslation(AUG_TRANSLATION, AUG_TRANSLATION, fill_mode=\"constant\", seed=SEED)\n",
    "\n",
    "# colorchanging augmentations according to http://arxiv.org/abs/1811.12231\n",
    "randSharp = keras_cv.layers.RandomSharpness(AUG_SHARPNESS, value_range=(0, 255), seed=SEED)\n",
    "randColorDegen = keras_cv.layers.RandomColorDegeneration(AUG_COLORDEGEN, seed=SEED)\n",
    "randNoise = tf.keras.layers.GaussianNoise(AUG_NOISE)\n",
    "\n",
    "def augment(img, label):\n",
    "  img = randRot(img)\n",
    "  img = randTrans(img)\n",
    "  img = randFlip(img)\n",
    "  img = randColorDegen(img)\n",
    "  img = randAugment(img)\n",
    "  img = randSharp(img)\n",
    "  img = randNoise(img)\n",
    "  return img, label\n",
    "\n",
    "if DEPICTION == \"body\":\n",
    "  trainDSphone = trainDSphone_raw.map(augment, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "trainDSmacro = trainDSmacro_raw.map(augment, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "trainDSmicro = trainDSmicro_raw.map(augment, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "valDS = valDS_raw.batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "testDS = testDS_raw.batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb4xgMLWgVaM"
   },
   "source": [
    "## Initialise Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3790,
     "status": "ok",
     "timestamp": 1713272066040,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -120
    },
    "id": "0qVsIxm-gVK0"
   },
   "outputs": [],
   "source": [
    "def build_model(BASE_MODEL, input_size):\n",
    "    # freeze all layers of the model\n",
    "    BASE_MODEL.trainable = False\n",
    "\n",
    "    # define classification head\n",
    "    flatten_layer = tf.keras.layers.GlobalAveragePooling2D()(BASE_MODEL.output)\n",
    "    regu_layer = tf.keras.layers.Dropout(DROPOUT)(flatten_layer)\n",
    "    output_layer = tf.keras.layers.Dense(4, activation = \"softmax\")(regu_layer)\n",
    "\n",
    "    # built model\n",
    "    model = tf.keras.Model(inputs=BASE_MODEL.inputs, outputs=output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "# load ImageNet pre-trained EfficientNet model and save them in dict\n",
    "model_dict = {}\n",
    "if DEPICTION == \"body\":\n",
    "    model_dict[\"phone\"]  = build_model(tf.keras.applications.EfficientNetV2B0(include_top=False, weights='imagenet', input_shape=(IMG_SIZE,IMG_SIZE,3), include_preprocessing=True), IMG_SIZE)\n",
    "model_dict[\"macro\"]  = build_model(tf.keras.applications.EfficientNetV2B0(include_top=False, weights='imagenet', input_shape=(IMG_SIZE,IMG_SIZE,3), include_preprocessing=True), IMG_SIZE)\n",
    "model_dict[\"micro\"]  = build_model(tf.keras.applications.EfficientNetV2B0(include_top=False, weights='imagenet', input_shape=(IMG_SIZE,IMG_SIZE,3), include_preprocessing=True), IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bR1RF7l0lHxL"
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "532779d1a6bb4d78b24126eb469ec967",
      "a57bcbd61a0e463c938392ebf765743c",
      "690232b55c2243528644009d815d2223",
      "1a6781ae26294da888b92f7a0d77e922",
      "cd56e7aa0de646458165c5b1c06229b0",
      "a3c55d4f16514f11a6b19f54b52bf3af",
      "6cd1c7dc7a6c410784b9fe20909fd027",
      "27a1d0d2e6c142e8aad3247d2b94f5ad"
     ]
    },
    "executionInfo": {
     "elapsed": 93110,
     "status": "ok",
     "timestamp": 1713272159132,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -120
    },
    "id": "NHWreqx_kmtC",
    "outputId": "fec8a9de-6ca3-4f4e-97f4-52bc82aa8607",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20240416_125425-16he4q9u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/convector/comparison_device/runs/16he4q9u' target=\"_blank\">macro_wing_3_2024-04-16-12-54</a></strong> to <a href='https://wandb.ai/convector/comparison_device' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/convector/comparison_device' target=\"_blank\">https://wandb.ai/convector/comparison_device</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/convector/comparison_device/runs/16he4q9u' target=\"_blank\">https://wandb.ai/convector/comparison_device/runs/16he4q9u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n",
      "35/35 [==============================] - 17s 190ms/step - loss: 1.4464 - accuracy: 0.2878 - val_loss: 1.2450 - val_accuracy: 0.5397\n",
      "Epoch 2/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 1.2531 - accuracy: 0.4263 - val_loss: 1.1321 - val_accuracy: 0.5900\n",
      "Epoch 3/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 1.1206 - accuracy: 0.5432 - val_loss: 1.0662 - val_accuracy: 0.6234\n",
      "Epoch 4/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 1.0375 - accuracy: 0.5899 - val_loss: 0.9997 - val_accuracy: 0.6276\n",
      "Epoch 5/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.9680 - accuracy: 0.6259 - val_loss: 0.9569 - val_accuracy: 0.6485\n",
      "Epoch 6/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.9307 - accuracy: 0.6655 - val_loss: 0.9350 - val_accuracy: 0.6695\n",
      "Epoch 7/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.8620 - accuracy: 0.6601 - val_loss: 0.9057 - val_accuracy: 0.6736\n",
      "Epoch 8/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.8565 - accuracy: 0.6673 - val_loss: 0.8875 - val_accuracy: 0.6736\n",
      "Epoch 9/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.8161 - accuracy: 0.7014 - val_loss: 0.8717 - val_accuracy: 0.6778\n",
      "Epoch 10/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.7693 - accuracy: 0.7248 - val_loss: 0.8495 - val_accuracy: 0.6946\n",
      "Epoch 11/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.7537 - accuracy: 0.7320 - val_loss: 0.8504 - val_accuracy: 0.6778\n",
      "Epoch 12/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.7379 - accuracy: 0.7320 - val_loss: 0.8317 - val_accuracy: 0.6904\n",
      "Epoch 13/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.6838 - accuracy: 0.7734 - val_loss: 0.8141 - val_accuracy: 0.6904\n",
      "Epoch 14/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.6671 - accuracy: 0.7626 - val_loss: 0.8155 - val_accuracy: 0.6946\n",
      "Epoch 15/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.6626 - accuracy: 0.7770 - val_loss: 0.7955 - val_accuracy: 0.7029\n",
      "Epoch 16/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.6419 - accuracy: 0.7662 - val_loss: 0.8000 - val_accuracy: 0.6987\n",
      "Epoch 17/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.6565 - accuracy: 0.7572 - val_loss: 0.7795 - val_accuracy: 0.7113\n",
      "Epoch 18/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.6259 - accuracy: 0.7698 - val_loss: 0.7697 - val_accuracy: 0.7113\n",
      "Epoch 19/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5994 - accuracy: 0.7932 - val_loss: 0.7511 - val_accuracy: 0.7238\n",
      "Epoch 20/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.6133 - accuracy: 0.7896 - val_loss: 0.7709 - val_accuracy: 0.7155\n",
      "Epoch 21/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5840 - accuracy: 0.7950 - val_loss: 0.7699 - val_accuracy: 0.7113\n",
      "Epoch 22/24\n",
      "35/35 [==============================] - 1s 25ms/step - loss: 0.5698 - accuracy: 0.8201 - val_loss: 0.7616 - val_accuracy: 0.6946\n",
      "Epoch 23/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5720 - accuracy: 0.7968 - val_loss: 0.7495 - val_accuracy: 0.7197\n",
      "Epoch 24/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5934 - accuracy: 0.7860 - val_loss: 0.7661 - val_accuracy: 0.7155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532779d1a6bb4d78b24126eb469ec967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.053 MB of 0.053 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██</td></tr><tr><td>loss</td><td>█▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▄▅▆▆▆▆▇▆▇▇▇▇▇█████▇██</td></tr><tr><td>val_loss</td><td>█▆▅▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.78597</td></tr><tr><td>best_epoch</td><td>22</td></tr><tr><td>best_val_loss</td><td>0.74947</td></tr><tr><td>epoch</td><td>23</td></tr><tr><td>loss</td><td>0.59343</td></tr><tr><td>val_accuracy</td><td>0.71548</td></tr><tr><td>val_loss</td><td>0.76608</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">macro_wing_3_2024-04-16-12-54</strong> at: <a href='https://wandb.ai/convector/comparison_device/runs/16he4q9u' target=\"_blank\">https://wandb.ai/convector/comparison_device/runs/16he4q9u</a><br/> View project at: <a href='https://wandb.ai/convector/comparison_device' target=\"_blank\">https://wandb.ai/convector/comparison_device</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_125425-16he4q9u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20240416_125517-ygrx2b9n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/convector/comparison_device/runs/ygrx2b9n' target=\"_blank\">micro_wing_3_2024-04-16-12-55</a></strong> to <a href='https://wandb.ai/convector/comparison_device' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/convector/comparison_device' target=\"_blank\">https://wandb.ai/convector/comparison_device</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/convector/comparison_device/runs/ygrx2b9n' target=\"_blank\">https://wandb.ai/convector/comparison_device/runs/ygrx2b9n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n",
      "35/35 [==============================] - 13s 160ms/step - loss: 1.3220 - accuracy: 0.3950 - val_loss: 0.9700 - val_accuracy: 0.6318\n",
      "Epoch 2/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.9091 - accuracy: 0.6248 - val_loss: 0.8886 - val_accuracy: 0.6485\n",
      "Epoch 3/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.8250 - accuracy: 0.6786 - val_loss: 0.8288 - val_accuracy: 0.6736\n",
      "Epoch 4/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.7737 - accuracy: 0.7289 - val_loss: 0.8247 - val_accuracy: 0.6695\n",
      "Epoch 5/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.7615 - accuracy: 0.7110 - val_loss: 0.7694 - val_accuracy: 0.7155\n",
      "Epoch 6/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.6874 - accuracy: 0.7397 - val_loss: 0.7373 - val_accuracy: 0.7197\n",
      "Epoch 7/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.6922 - accuracy: 0.7289 - val_loss: 0.6990 - val_accuracy: 0.7448\n",
      "Epoch 8/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.6438 - accuracy: 0.7630 - val_loss: 0.6999 - val_accuracy: 0.7280\n",
      "Epoch 9/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.6238 - accuracy: 0.7522 - val_loss: 0.7249 - val_accuracy: 0.7197\n",
      "Epoch 10/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.6458 - accuracy: 0.7469 - val_loss: 0.6837 - val_accuracy: 0.7322\n",
      "Epoch 11/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.6271 - accuracy: 0.7558 - val_loss: 0.6844 - val_accuracy: 0.7364\n",
      "Epoch 12/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5931 - accuracy: 0.7935 - val_loss: 0.6634 - val_accuracy: 0.7490\n",
      "Epoch 13/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.6016 - accuracy: 0.7756 - val_loss: 0.6857 - val_accuracy: 0.7280\n",
      "Epoch 14/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5535 - accuracy: 0.8133 - val_loss: 0.6733 - val_accuracy: 0.7322\n",
      "Epoch 15/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5797 - accuracy: 0.7935 - val_loss: 0.6682 - val_accuracy: 0.7280\n",
      "Epoch 16/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5655 - accuracy: 0.7971 - val_loss: 0.6669 - val_accuracy: 0.7280\n",
      "Epoch 17/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5596 - accuracy: 0.7774 - val_loss: 0.6606 - val_accuracy: 0.7364\n",
      "Epoch 18/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5725 - accuracy: 0.7989 - val_loss: 0.6607 - val_accuracy: 0.7364\n",
      "Epoch 19/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5606 - accuracy: 0.8007 - val_loss: 0.6474 - val_accuracy: 0.7406\n",
      "Epoch 20/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5399 - accuracy: 0.8169 - val_loss: 0.6362 - val_accuracy: 0.7364\n",
      "Epoch 21/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5664 - accuracy: 0.7882 - val_loss: 0.6481 - val_accuracy: 0.7364\n",
      "Epoch 22/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5196 - accuracy: 0.8187 - val_loss: 0.6367 - val_accuracy: 0.7280\n",
      "Epoch 23/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5390 - accuracy: 0.8133 - val_loss: 0.6407 - val_accuracy: 0.7322\n",
      "Epoch 24/24\n",
      "35/35 [==============================] - 1s 26ms/step - loss: 0.5071 - accuracy: 0.8097 - val_loss: 0.6242 - val_accuracy: 0.7364\n"
     ]
    }
   ],
   "source": [
    "LOSS = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "OPTIMIZER_TRANSFER = tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE_TRANSFER)\n",
    "EARLY_STOP = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = EARLY_STOP_PATIENCE, mode='auto', restore_best_weights=False)\n",
    "\n",
    "for model in model_dict:\n",
    "  wandb.finish()\n",
    "  model_run = \"_\".join([model, DEPICTION, str(SEED), datetime.now().strftime('%Y-%m-%d-%H-%M')])\n",
    "  # Inititiliaze weights&biases\n",
    "  wandb.init(name=model_run,\n",
    "             project=\"comparison_device\",\n",
    "\n",
    "      # track hyperparameters and run metadata with wandb.config\n",
    "      config={\n",
    "          \"Model Name\": model,\n",
    "          \"Depiction\": DEPICTION,\n",
    "          \"Batch Size\": BATCH_SIZE,\n",
    "          \"Epochs\": MAX_EPOCHS_TRANSFER,\n",
    "          \"Learning Rate\": LEARNING_RATE_TRANSFER})\n",
    "\n",
    "  MONITOR_CALLBACK =  wandb.keras.WandbCallback(save_model=False)\n",
    "\n",
    "  if \"macro\" in model:\n",
    "      trainDS = trainDSmacro\n",
    "  elif \"micro\" in model:\n",
    "      trainDS = trainDSmicro\n",
    "  elif \"phone\" in model and DEPICTION == \"body\":\n",
    "      trainDS = trainDSphone\n",
    "\n",
    "  model_dict[model].compile(optimizer=OPTIMIZER_TRANSFER, loss=LOSS, metrics=[\"accuracy\"])\n",
    "  history = model_dict[model].fit(trainDS, epochs=MAX_EPOCHS_TRANSFER, validation_data=valDS, callbacks=[EARLY_STOP, MONITOR_CALLBACK])\n",
    "\n",
    "  # save model as .h5\n",
    "  model_dict[model].save(os.path.join(SAVE_DIR, model_run+\".h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test models on validation and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 12932,
     "status": "ok",
     "timestamp": 1713272172050,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -120
    },
    "id": "vTEDEi9y3JoH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cnn_prediction(cnn_model, dataset):\n",
    "    # get model prediction\n",
    "    prediction_list = cnn_model.predict(dataset, verbose=0)\n",
    "\n",
    "    def parse_prediction(prediction_list, rank):\n",
    "        highest_score_list = [np.sort(prediction)[-rank] for prediction in prediction_list]\n",
    "        highest_score_index_list = [np.where(prediction_list[i] == highest_score)[0][0] for i, highest_score in enumerate(highest_score_list)]\n",
    "        species_name_list = [species_list[highest_score_index] for highest_score_index in highest_score_index_list]\n",
    "\n",
    "        return np.asarray(highest_score_list), np.asarray(species_name_list)\n",
    "\n",
    "    highest_score_list, species_name_list = parse_prediction(prediction_list, 1)\n",
    "    sec_highest_score_list, sec_species_name_list = parse_prediction(prediction_list, 2)\n",
    "\n",
    "    return highest_score_list, species_name_list, sec_highest_score_list, sec_species_name_list, prediction_list\n",
    "\n",
    "def get_system_prediction(cnn_model, designation, species_list):\n",
    "    # load images\n",
    "    df_temp = pd.DataFrame(columns=[\"image_path\", \"pred_label\", \"highest_score\", \"sec_pred_label\", \"sec_highest_score\", \"datasplit\", \"true_label\"])\n",
    "    for species in species_list:\n",
    "        try:\n",
    "            folder_path = os.path.join(DIR_PATH, designation, species, \"*.png\")\n",
    "            file_list = tf.data.Dataset.list_files(folder_path, shuffle=False)\n",
    "            dataset = file_list.map(getImage).batch(1)\n",
    "\n",
    "            # write predictions to dataframe\n",
    "            highest_score_list, species_name_list, sec_highest_score_list, sec_species_name_list, prediction_list = get_cnn_prediction(cnn_model, dataset)\n",
    "\n",
    "            \n",
    "            df = pd.DataFrame({\"image_path\": [f.numpy().decode('utf-8') for f in file_list],\n",
    "                           \"pred_label\": species_name_list,\n",
    "                           \"highest_score\": highest_score_list,\n",
    "                           \"sec_pred_label\": sec_species_name_list,\n",
    "                           \"sec_highest_score\": sec_highest_score_list,\n",
    "                               \"datasplit\": [designation]*file_list.__len__().numpy(),\n",
    "                               \"true_label\": [species]*file_list.__len__().numpy(),})\n",
    "\n",
    "            df_temp = pd.concat([df_temp, df])\n",
    "\n",
    "        except Exception as e: print(species , e)\n",
    "    return df_temp\n",
    "\n",
    "df_dict = {}\n",
    "for model in model_dict.keys():\n",
    "    model_run = \"_\".join([model, DEPICTION, str(SEED), datetime.now().strftime('%Y-%m-%d-%H-%M')])\n",
    "    df_val = get_system_prediction(model_dict[model], \"val\", species_list)\n",
    "    df_test = get_system_prediction(model_dict[model], \"test\", species_list)\n",
    "    df_dict[model] = pd.concat([df_val, df_test])\n",
    "\n",
    "    df_dict[model][\"correct\"] = df_dict[model][\"pred_label\"] == df_dict[model][\"true_label\"]\n",
    "    df_dict[model].to_pickle(os.path.join(SAVE_DIR, model_run + \"_VALTEST.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "venv-metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1a6781ae26294da888b92f7a0d77e922": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27a1d0d2e6c142e8aad3247d2b94f5ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "532779d1a6bb4d78b24126eb469ec967": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a57bcbd61a0e463c938392ebf765743c",
       "IPY_MODEL_690232b55c2243528644009d815d2223"
      ],
      "layout": "IPY_MODEL_1a6781ae26294da888b92f7a0d77e922"
     }
    },
    "690232b55c2243528644009d815d2223": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6cd1c7dc7a6c410784b9fe20909fd027",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_27a1d0d2e6c142e8aad3247d2b94f5ad",
      "value": 1
     }
    },
    "6cd1c7dc7a6c410784b9fe20909fd027": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3c55d4f16514f11a6b19f54b52bf3af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a57bcbd61a0e463c938392ebf765743c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd56e7aa0de646458165c5b1c06229b0",
      "placeholder": "​",
      "style": "IPY_MODEL_a3c55d4f16514f11a6b19f54b52bf3af",
      "value": "0.066 MB of 0.066 MB uploaded\r"
     }
    },
    "cd56e7aa0de646458165c5b1c06229b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
